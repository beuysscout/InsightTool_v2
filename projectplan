# Insight Tool â€” Research Plan & Build Strategy

---

## Purpose

Build an AI-assisted tool that analyses customer interview transcripts and/or videos to produce robust, evidence-backed insights for product design opportunities.

---

## Research Process

Five steps from raw inputs to finished report. The researcher uploads and reviews; AI does the analytical heavy lifting.

| Step | Activity | What happens |
|---|---|---|
| 1 | **Upload Research Guide** | Researcher uploads their research guide. AI analyses the guide structure â€” identifies sections, questions, and objectives â€” so it can use this as the framework for all downstream analysis. |
| 2 | **Upload & Organise Transcripts** | Researcher uploads raw interview transcripts. AI parses them into speaker turns and organises responses against the corresponding guide sections and questions. Researcher reviews the mappings. |
| 3 | **Theme Analysis** | AI works through each organised transcript individually and surfaces emergent themes with supporting quotes. Researcher reviews, merges, or discards themes per transcript. |
| 4 | **Insight Synthesis** | AI merges themes across all interview transcripts â€” identifies shared patterns, common pains, and alike themes. Generates candidate insights: a one-sentence summary capturing the essence, a hero quote that illustrates it, and clear citations with source trail. Researcher reviews and edits. |
| 5 | **Insight & Recommendation Report** | AI compiles the final deliverable: robust evidence trail, concise insight summaries, and useful visualisations. Researcher finalises before delivery. |

---

## How AI Works With Us

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  STEP 1 â€” UPLOAD RESEARCH GUIDE                              â•‘
â•‘                                                              â•‘
â•‘  Researcher uploads their guide                              â•‘
â•‘         â”‚                                                    â•‘
â•‘         â””â”€â”€â–º AI analyses guide structure                      â•‘
â•‘              Identifies sections, questions, objectives       â•‘
â•‘              This becomes the framework for all analysis      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                          â”‚
                          â–¼  [Interviews run on Askable]
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  STEP 2 â€” UPLOAD & ORGANISE TRANSCRIPTS                      â•‘
â•‘                                                              â•‘
â•‘  Researcher uploads raw transcripts                          â•‘
â•‘         â”‚                                                    â•‘
â•‘         â”œâ”€â”€â–º AI parses into speaker turns                     â•‘
â•‘         â”‚                                                    â•‘
â•‘         â””â”€â”€â–º AI organises responses against guide             â•‘
â•‘              sections and questions                           â•‘
â•‘              Researcher reviews mappings                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                          â”‚
                          â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  STEP 3 â€” THEME ANALYSIS  (per transcript)                   â•‘
â•‘                                                              â•‘
â•‘  For each organised transcript:                              â•‘
â•‘         â”‚                                                    â•‘
â•‘         â””â”€â”€â–º AI surfaces emergent themes with                 â•‘
â•‘              supporting quotes                                â•‘
â•‘              Researcher reviews, merges, or discards          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                          â”‚
                          â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  STEP 4 â€” INSIGHT SYNTHESIS  (across all transcripts)        â•‘
â•‘                                                              â•‘
â•‘  AI merges themes across all interviews                      â•‘
â•‘         â”‚                                                    â•‘
â•‘         â”œâ”€â”€â–º Shared patterns, common pains, alike themes      â•‘
â•‘         â”‚                                                    â•‘
â•‘         â”œâ”€â”€â–º Candidate insights: one-sentence summary         â•‘
â•‘         â”‚    + hero quote that illustrates it                 â•‘
â•‘         â”‚                                                    â•‘
â•‘         â””â”€â”€â–º Citations and source trail for each              â•‘
â•‘                                                              â•‘
â•‘  Researcher reviews, edits, and approves                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                          â”‚
                          â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  STEP 5 â€” INSIGHT & RECOMMENDATION REPORT                    â•‘
â•‘                                                              â•‘
â•‘  AI compiles final deliverable:                              â•‘
â•‘         â”‚                                                    â•‘
â•‘         â”œâ”€â”€â–º Robust evidence trail                            â•‘
â•‘         â”œâ”€â”€â–º Concise insight summaries                        â•‘
â•‘         â””â”€â”€â–º Useful visualisations                            â•‘
â•‘                                                              â•‘
â•‘  Researcher reviews and finalises before delivery            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## User Flow & Interface Design

### Interface Types

Two modes are used throughout the tool â€” chosen based on the nature of the task at each stage:

| Type | When used | Why |
|---|---|---|
| **Structured view** | Displaying and editing codebooks, guides, annotated transcripts, heatmaps | Researcher needs to scan, compare, and edit structured data efficiently |
| **Conversational UI** | Human review gate, codebook curation, next session prep, synthesis Q&A | Researcher needs to explain a judgement call in plain language; system must interpret and apply it |

---

### Overall Journey Map

```
STUDY SETUP        PRE-RESEARCH          PER SESSION           CROSS-SESSION         SYNTHESIS          REPORT
    â”‚                    â”‚                    â”‚                      â”‚                    â”‚                 â”‚
Create study    Brief â†’ Guide â†’         Upload transcript      View heatmap         Review insights   Preview &
                Codebook seed           Anonymise + analyse    Curate codebook      Edit statements   export
                                        Human review gate      Saturation check
                                        Next session prep
                                             â”‚
                                        [repeat per session]
```

---

### Stage 0 â€” Study Setup

**Interface type:** Structured form

**Screen: New Study**

| Field | Input type | Notes |
|---|---|---|
| Study name | Text | e.g. "Onboarding â€” Feb 2026" |
| Product area | Text or select | Context for AI throughout |
| HMW statements | Multi-line text | "How might we..." framing |
| Research objectives | Multi-line text | The questions we need to answer |
| Participant criteria | Text | Who is being recruited |
| Target session count | Number | How many interviews planned |
| Participant ID scheme | Radio | Auto-generated (P01, P02...) or custom prefix |
| AI analysis consent | Checkbox | Researcher confirms participants were informed this interview may be analysed by AI |

**Actions:**
- Save draft
- Generate Brief â€” triggers Brief Agent, produces structured YAML research brief
- View generated brief (read-only preview, researcher confirms or edits)

**Human judgement moment:** Researcher reviews the structured brief the AI has generated from their freeform inputs and confirms it accurately represents their intent before any downstream agents use it as ground truth.

---

### Stage 1 â€” Guide Review

**Interface type:** Structured view with inline edit

**Screen: Interview Guide**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Section: Core Questions                    [+ Add section] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  OB1  Walk me through your first login         [Required]   â”‚
â”‚       Mapped to: Objective 1                               â”‚
â”‚       Probes: "What did you expect to see?" /              â”‚
â”‚       "Were you surprised by anything?"                    â”‚
â”‚       [Edit]  [Delete]  [Mark optional]                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš ï¸  OB3  Do you prefer X or Y?               [Flagged]    â”‚
â”‚       Issue: Leading question â€” assumes preference exists  â”‚
â”‚       Suggestion: "How do you think about X?"              â”‚
â”‚       [Accept suggestion]  [Edit manually]  [Dismiss]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Validation panel (right sidebar):**
- Questions mapped to objectives â€” any objectives uncovered?
- Flagged questions (leading, ambiguous, out of scope)
- Estimated session duration based on question count

**Actions:**
- Edit any question inline
- Accept / dismiss AI flags
- Reorder sections (drag)
- Add / delete questions
- Lock guide â€” creates v1 baseline; any subsequent changes are versioned and flagged in cross-session analysis

**Human judgement moment:** Researcher locks the guide. This is a commitment â€” any changes after this point are tracked as a new version and surfaced in cross-session analysis (e.g. "Q added after session 2").

---

### Stage 2 â€” Codebook Review

**Interface type:** Structured view with inline edit

**Screen: Codebook**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Theme: NAVIGATION CONFUSION            [Deductive]  v1        â”‚
â”‚                                                                â”‚
â”‚  Codes                                                         â”‚
â”‚  â”œâ”€â”€ CONFUSION_NAV                                             â”‚
â”‚  â”‚     Definition: Participant expresses difficulty            â”‚
â”‚  â”‚     locating a UI element or understanding layout           â”‚
â”‚  â”‚     Indicators: "couldn't find", "kept clicking",           â”‚
â”‚  â”‚     "didn't know where"                                     â”‚
â”‚  â”‚     Example: "I just kept clicking different tabs"          â”‚
â”‚  â”‚     [Edit]  [Delete]                                        â”‚
â”‚  â””â”€â”€ CONFUSION_MENTAL_MODEL                                    â”‚
â”‚        Definition: Participant's expectation of system         â”‚
â”‚        behaviour does not match actual behaviour               â”‚
â”‚        [Edit]  [Delete]                                        â”‚
â”‚                                                                â”‚
â”‚  [+ Add code]                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Actions:**
- Edit code definition, indicators, example quotes
- Add / delete codes
- Drag codes between themes
- Add new theme
- Lock codebook â€” creates v1 baseline

**Human judgement moment:** Researcher confirms the codebook accurately reflects their hypotheses before the first session runs deductive coding against it.

---

### Stage 3 â€” Transcript Upload (Per Session)

**Interface type:** Upload â†’ anonymisation review â†’ progress

This is a three-step sequence. No AI analysis begins until anonymisation is confirmed.

**Step 1 â€” Upload**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Session 3 of 8  Â·  Participant: P03                â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Drag transcript file here                  â”‚   â”‚
â”‚  â”‚  or click to browse                         â”‚   â”‚
â”‚  â”‚  .vtt  .srt  .txt  .json                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                     â”‚
â”‚  Detected format: VTT                               â”‚
â”‚  Duration: 52 min  Â·  Turns: 184                    â”‚
â”‚                                                     â”‚
â”‚  [Scan for personal information]                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step 2 â€” Anonymisation review (human gate â€” see Privacy section)**

Researcher reviews and confirms all PII redactions before any AI call is made. Original transcript is discarded at the end of this step.

**Step 3 â€” Processing progress**

```
  âœ“  Format detected â€” VTT
  âœ“  Parsed to 184 turns
  âœ“  Anonymised â€” original discarded
  â—  Guide coverage analysis...
  â—  Deductive coding...         â† parallel
     Inductive pass  (waiting)
     Session QA  (waiting)
```

---

### Stage 4 â€” Session Analysis (Per Session)

**Interface type:** Multi-panel structured view

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GUIDE COVERAGE  â”‚  ANNOTATED TRANSCRIPT    â”‚  CODES              â”‚
â”‚                  â”‚                          â”‚                     â”‚
â”‚  OB1  â–ˆâ–ˆâ–ˆâ–ˆ  4/5  â”‚  [00:12:34] [PARTICIPANT]â”‚  Active filters:    â”‚
â”‚  OB2  â–ˆâ–ˆ    2/5  â”‚  "I just kept clicking   â”‚  All codes  â–¼       â”‚
â”‚  OB3  â–ˆâ–ˆâ–ˆâ–ˆ  4/5  â”‚   on different tabs."    â”‚                     â”‚
â”‚  FD1  â€”     â€”    â”‚  CONFUSION_NAV  â˜…â˜…â˜…      â”‚  CONFUSION_NAV  12  â”‚
â”‚  FD2  â–ˆâ–ˆâ–ˆ   3/5  â”‚  WORKAROUND_SELF  â˜…â˜…     â”‚  WORKAROUND  8      â”‚
â”‚                  â”‚  OB2  depth 3/5          â”‚  DELIGHT  2         â”‚
â”‚  Skipped: FD1    â”‚                          â”‚  [EMERGENT]         â”‚
â”‚  [See why]       â”‚  [00:13:15] [PARTICIPANT]â”‚  export_pain  4     â”‚
â”‚                  â”‚  "I honestly just        â”‚                     â”‚
â”‚  Coverage: 80%   â”‚   Googled it."           â”‚                     â”‚
â”‚  Avg depth: 3.2  â”‚  WORKAROUND_EXT  â˜…â˜…â˜…    â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

All quotes shown are already anonymised â€” participant names replaced with `[PARTICIPANT]` etc. prior to this view.

**Actions â€” Transcript panel:**
- Click any turn to see its full coding detail
- Filter turns by code, depth score, confidence level, or guide question
- Flag a turn as noise (exclude from analysis)
- Add a manual code to a turn

**Actions â€” Coverage panel:**
- Click any question to jump to the first turn that covers it
- See depth breakdown (specificity / elaboration / emotional salience / actionability)
- See off-script classification for uncovered segments

**Actions â€” Codes panel:**
- Filter transcript by clicking a code
- Hover to see definition

---

### Stage 5 â€” Human Review Gate (Per Session)

**Interface type:** Conversational panel

Triggered automatically after Session QA Agent runs. Appears as a slide-in panel over the analysis screen. Nothing is committed to StudyState until the researcher completes this step.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Review â€” Session 3                              3 items       â”‚
â”‚                                                                â”‚
â”‚  I've flagged 3 codes for your review before committing        â”‚
â”‚  this session's results.                                       â”‚
â”‚                                                                â”‚
â”‚  1 / 3                                                         â”‚
â”‚                                                                â”‚
â”‚  [00:24:11]  "It sort of reminded me of how my bank app works" â”‚
â”‚                                                                â”‚
â”‚  Proposed: CONFUSION_MENTAL_MODEL  (confidence 0.51)          â”‚
â”‚                                                                â”‚
â”‚  I'm uncertain â€” this could be a mental model mismatch,        â”‚
â”‚  or just an analogy. How do you read it?                      â”‚
â”‚                                                                â”‚
â”‚  [Confirm code]  [Change to...]  [Mark as noise]  [Skip]      â”‚
â”‚                                                                â”‚
â”‚  â”€â”€ or type a response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  > That's actually DELIGHT â€” they liked the familiarity       â”‚
â”‚                                                                â”‚
â”‚  Got it â€” coded as DELIGHT (confidence overridden by          â”‚
â”‚  researcher). Moving to item 2.                                â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Actions:**
- Confirm proposed code
- Select alternative code from list
- Type a free-text correction (Claude interprets and applies)
- Mark as noise (no code)
- Skip (leave pending for later)
- Commit all reviewed codes and continue

**Human judgement moment:** The most important gate in the system. Nothing is written to StudyState until the researcher has reviewed flagged items.

---

### Stage 6 â€” Next Session Prep (Per Session)

**Interface type:** Conversational brief + structured summary

Shown after the human review gate is completed. Researcher reads this before their next interview.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Session 4 Prep Brief                                          â”‚
â”‚                                                                â”‚
â”‚  â”€â”€ Push harder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚  OB2 (avg depth 2.1/5 across 3 sessions)                      â”‚
â”‚  The probe "Did you look for help anywhere?" worked in         â”‚
â”‚  session 3 but wasn't used in 1 or 2. Use it.                  â”‚
â”‚                                                                â”‚
â”‚  â”€â”€ Consistently skipped â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚  FD1 â€” skipped in all 3 sessions. Consider moving it           â”‚
â”‚  earlier in the guide before OB2 runs long.                    â”‚
â”‚                                                                â”‚
â”‚  â”€â”€ Worth testing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚  EMERGENT: export_pain has appeared in 3/3 sessions            â”‚
â”‚  but isn't in your codebook or guide. Probe for it             â”‚
â”‚  directly â€” try: "Tell me about a time you tried to get        â”‚
â”‚  information out of the product."                              â”‚
â”‚                                                                â”‚
â”‚  [Ask a question]          [Dismiss]  [Export as PDF]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Actions:**
- Ask follow-up questions in natural language
- Dismiss / archive brief
- Export as PDF to take into the Askable session

---

### Stage 7 â€” Cross-Session Dashboard

**Interface type:** Structured views with conversational sidebar

Accessible at any point after session 2. Updates after each committed session.

**Screen: Heatmap**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Theme Heatmap  Â·  Sessions 1â€“5 of 8                         â”‚
â”‚                                                               â”‚
â”‚  Filter by: All themes â–¼   All segments â–¼                    â”‚
â”‚                                                               â”‚
â”‚                      P01  P02  P03  P04  P05                  â”‚
â”‚  CONFUSION_NAV        â–ˆâ–ˆ   â–ˆâ–ˆ   â–ˆ    â–ˆâ–ˆ   â–ˆâ–ˆ                  â”‚
â”‚  CONFUSION_MM         â–ˆâ–ˆ   â–ˆ    â–ˆâ–ˆ   â€”    â–ˆ                   â”‚
â”‚  WORKAROUND_EXT       â–ˆ    â€”    â–ˆâ–ˆ   â–ˆ    â€”                   â”‚
â”‚  DELIGHT              â€”    â–ˆ    â€”    â€”    â–ˆ                   â”‚
â”‚  EXPORT_PAIN *        â–ˆ    â–ˆâ–ˆ   â€”    â–ˆâ–ˆ   â–ˆâ–ˆ   â† emergent     â”‚
â”‚                                                               â”‚
â”‚  * Not yet in codebook                                        â”‚
â”‚                                                               â”‚
â”‚  [Add EXPORT_PAIN to codebook]  [Ask about this pattern]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Participant references are always IDs (P01, P02...) â€” never real names.

**Screen: Saturation**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Saturation Tracker                                           â”‚
â”‚                                                               â”‚
â”‚  New themes per session                                       â”‚
â”‚  S1  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  6 new                                      â”‚
â”‚  S2  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        3 new                                      â”‚
â”‚  S3  â–ˆâ–ˆâ–ˆâ–ˆ          2 new                                      â”‚
â”‚  S4  â–ˆâ–ˆ            1 new                                      â”‚
â”‚  S5  â–ˆ             1 new                                      â”‚
â”‚                                                               â”‚
â”‚  âš¡  Rate is declining. If session 6 produces 0â€“1 new         â”‚
â”‚     themes, you may have reached saturation.                  â”‚
â”‚                                                               â”‚
â”‚  [Continue to session 6]   [Stop collecting, go to synthesis] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Actions:**
- Filter heatmap by theme, participant, session range
- Click any cell to see supporting (anonymised) quotes
- Accept saturation signal â†’ transitions study to Synthesis phase
- Ask cross-session questions in conversational sidebar ("Which participants showed both CONFUSION_NAV and WORKAROUND_EXT?")

---

### Stage 8 â€” Codebook Curation

**Interface type:** Conversational panel

Runs every N sessions or on-demand.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Codebook Review  Â·  After session 5            4 items        â”‚
â”‚                                                                â”‚
â”‚  â”€â”€ ADD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚  EXPORT_PAIN â€” appeared in 4/5 sessions, high salience.       â”‚
â”‚  Suggested definition: "Participant expresses frustration      â”‚
â”‚  or friction when attempting to extract data or content        â”‚
â”‚  from the product."                                            â”‚
â”‚  [Accept]  [Edit definition]  [Reject]                        â”‚
â”‚                                                                â”‚
â”‚  â”€â”€ SPLIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚  CONFUSION_NAV is being applied to two distinct patterns:      â”‚
â”‚  navigation layout confusion vs. information architecture.     â”‚
â”‚  Suggested: CONFUSION_NAV_LAYOUT + CONFUSION_NAV_IA           â”‚
â”‚  [Accept split]  [Keep as one]  [Discuss]                     â”‚
â”‚                                                                â”‚
â”‚  â”€â”€ RETIRE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚  DELIGHT â€” only 2/5 sessions, confidence consistently low.    â”‚
â”‚  Suggested: collapse into POSITIVE_MOMENT (broader).          â”‚
â”‚  [Accept]  [Keep]                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Actions:**
- Accept / reject each recommendation individually
- Edit suggested definitions inline before accepting
- Ask for reasoning in natural language
- Apply approved changes â†’ bumps codebook to next version, change logged in history

---

### Stage 9 â€” Synthesis

**Interface type:** Structured list + conversational sidebar

**Screen: Insights**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Candidate Insights                    Sort: Frequency â–¼       â”‚
â”‚                                                                â”‚
â”‚  â”€â”€ Insight 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  Participants consistently find workarounds rather than        â”‚
â”‚  contacting support when stuck.                                â”‚
â”‚                                                                â”‚
â”‚  Evidence: 5/5 participants  Â·  12 coded instances             â”‚
â”‚  Codes: WORKAROUND_SELF + WORKAROUND_EXT                       â”‚
â”‚  Salience: High  Â·  Objective alignment: OB2 âœ“                 â”‚
â”‚                                                                â”‚
â”‚  Supporting quotes (3 shown of 12)  [Expand]                  â”‚
â”‚  "I honestly just Googled it." â€” P01                          â”‚
â”‚  "I asked a colleague, I didn't think there was help          â”‚
â”‚   in the app." â€” P03                                          â”‚
â”‚                                                                â”‚
â”‚  [Accept]  [Edit statement]  [Reject]  [Merge with...]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

All quotes attributed to participant IDs only.

**Actions:**
- Accept / edit / reject each insight
- Reorder accepted insights (drag)
- Merge two insights into one
- Ask cross-session questions ("Are there participants who bucked this pattern?")
- Promote accepted insights to report

---

### Stage 10 â€” Report

**Interface type:** Structured document view + export

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Research Report â€” Onboarding Feb 2026                        â”‚
â”‚                                                                â”‚
â”‚  KEY INSIGHTS                                                  â”‚
â”‚  1. Users default to self-service workarounds over in-product  â”‚
â”‚     help â€” indicating a trust deficit in the help system.      â”‚
â”‚     [Edit]                                                     â”‚
â”‚                                                                â”‚
â”‚  OBSERVATIONS                                                  â”‚
â”‚  - FD1 was consistently skipped â€” consider restructuring the   â”‚
â”‚    guide section order for future studies.                     â”‚
â”‚  - EXPORT_PAIN emerged in 4/5 sessions and was not in the      â”‚
â”‚    original brief â€” warrants a dedicated follow-up study.      â”‚
â”‚                                                                â”‚
â”‚  RECOMMENDATIONS  ...                                          â”‚
â”‚                                                                â”‚
â”‚  [Export PDF]  [Export DOCX]  [Export annotated transcript]   â”‚
â”‚  [Export codebook YAML]  [Export heatmap CSV]                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Actions:**
- Edit any section inline
- Reorder insights
- Export in multiple formats (all exports use anonymised data)

---

### Navigation Structure

```
STUDIES
â””â”€â”€ [Study name]
    â”œâ”€â”€ Brief
    â”œâ”€â”€ Guide             (locked after first session, versioned)
    â”œâ”€â”€ Codebook          (versioned â€” v1, v2...)
    â”œâ”€â”€ Sessions
    â”‚   â”œâ”€â”€ Session 1     (upload â†’ anonymise â†’ analyse â†’ review â†’ prep)
    â”‚   â”œâ”€â”€ Session 2
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ Cross-Session
    â”‚   â”œâ”€â”€ Heatmap
    â”‚   â”œâ”€â”€ Saturation
    â”‚   â””â”€â”€ Codebook Curation
    â”œâ”€â”€ Insights
    â”œâ”€â”€ Report
    â””â”€â”€ Data & Privacy
```

---

### Key Human Judgement Moments

These are the gates that must be designed to feel trustworthy, not bureaucratic:

| Gate | Stage | What the researcher decides |
|---|---|---|
| Brief confirmation | Pre-research | Does this accurately represent my intent? |
| Guide lock | Pre-research | Am I happy to run interviews against this? |
| Codebook lock | Pre-research | Do my hypotheses map to these codes? |
| Anonymisation review | Per session | Is the PII detection correct before the original is discarded? |
| Per-session code review | Per session | Are these flagged codes correct? |
| Saturation decision | Cross-session | Do I have enough data to stop collecting? |
| Codebook curation | Cross-session | What changes does the evidence support? |
| Insight approval | Synthesis | Which of these statements do I stand behind? |

Every one of these should be conversational, not a form. The researcher should be able to explain their decision in plain language and have the system interpret it correctly.

---

## Privacy & Data Security

### Core Principles

These are design constraints, not features. They apply to every stage and every agent call:

- **Anonymise before AI sees anything** â€” the original transcript is never sent to an LLM. Only the anonymised version is processed.
- **No real names anywhere in the system** â€” participants are referenced by generated IDs only (P01, P02...).
- **Quotes propagate anonymised** â€” every quote in reviews, insights, and reports is already stripped of PII.
- **Original is transient** â€” the raw transcript is processed locally and discarded after the researcher confirms anonymisation. Only the anonymised version is stored.
- **Researcher holds the ID mapping** â€” the tool generates participant IDs but does not store the mapping between those IDs and real people. That mapping stays with the researcher.
- **Researcher controls retention** â€” explicit delete-study action purges all associated data.

---

### Anonymisation Pipeline

The upload flow is a strict two-step sequence. No AI analysis begins until the researcher has confirmed anonymisation:

```
Upload transcript
      â”‚
      â–¼
PII scan (local â€” before any AI call)
      â”‚
      â–¼
Anonymisation review â† HUMAN GATE
      â”‚
      â–¼
Discard original â† PERMANENT
      â”‚
      â–¼
AI analysis runs on anonymised text only
      â”‚
      â–¼
All downstream: quotes, insights, report â€” anonymised throughout
```

---

### Anonymisation Review Screen

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Anonymisation review  Â·  Session 3                             â”‚
â”‚                                                                 â”‚
â”‚  Found 14 items that may contain personal information.          â”‚
â”‚  Review each and confirm before analysis runs.                  â”‚
â”‚  The original file will be permanently discarded after this.    â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Auto-redacted (high confidence) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  âœ“  "My name is Sarah" â†’ "My name is [PARTICIPANT]"            â”‚
â”‚  âœ“  "sarah@example.com" â†’ "[EMAIL]"                            â”‚
â”‚  âœ“  [INTERVIEWER] applied to all researcher turns               â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Needs your decision (low confidence) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚                                                                 â”‚
â”‚  1 / 3                                                          â”‚
â”‚  "...my manager John kept asking about it..."                   â”‚
â”‚  Possible third-party name detected: "John"                     â”‚
â”‚                                                                 â”‚
â”‚  [Redact â†’ "my manager [NAME]"]   [Keep as-is]                 â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sensitive content flagged â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  [00:34:22]  Participant disclosed a health condition.          â”‚
â”‚  This segment has been auto-excluded from coding.              â”‚
â”‚  [Review segment]  [Confirm exclusion]                         â”‚
â”‚                                                                 â”‚
â”‚  [Confirm all & discard original]                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### PII Detection Table

| PII type | Replacement token | Default behaviour |
|---|---|---|
| Participant's name | `[PARTICIPANT]` | Auto-redact if high confidence |
| Interviewer's name | `[INTERVIEWER]` | Auto-redact always |
| Third-party names | `[NAME]` | Flag for researcher review |
| Email addresses | `[EMAIL]` | Auto-redact always |
| Phone numbers | `[PHONE]` | Auto-redact always |
| Company names | `[COMPANY]` | Flag for researcher review |
| Locations (city, suburb) | `[LOCATION]` | Flag for researcher review |
| Sensitive disclosures | Segment excluded | Flag for researcher review |

**Implementation:** Microsoft Presidio (`presidio-analyzer`) for PII detection, run locally before any network call. High-confidence detections (score > 0.85) are auto-redacted; lower-confidence are surfaced for researcher review.

---

### Data Storage Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STORED                               NOT STORED                  â”‚
â”‚                                                                   â”‚
â”‚  âœ“ Anonymised transcript              âœ— Original transcript        â”‚
â”‚  âœ“ Participant IDs (P01, P02...)       âœ— Participant real names    â”‚
â”‚  âœ“ Codes applied to turns             âœ— Askable profile data       â”‚
â”‚  âœ“ Anonymised quotes                  âœ— Sensitive disclosures      â”‚
â”‚  âœ“ Codebook + guide (versioned)       âœ— Recruiter/screener data    â”‚
â”‚  âœ“ Insights + report                  âœ— ID-to-name mapping         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### StudyState â€” Privacy-Relevant Fields

The `sessions` entry in StudyState stores only the anonymised transcript. The original file path is never persisted:

```yaml
sessions:
  - session_id: str
    participant_id: str          # e.g. "P03" â€” not a real name
    transcript: Turn[]           # anonymised turns only â€” original discarded
    anonymisation_log:           # audit record of what was redacted
      auto_redacted: int         # count of auto-redactions
      researcher_reviewed: int   # count of items reviewed by researcher
      exclusions: int            # segments excluded (sensitive disclosures)
    coverage_result: QuestionCoverageResult[]
    coded_turns: CodedTurn[]
    emergent_themes: EmergentTheme[]
    quality_scorecard: SessionScorecard
    review_status: pending | approved | rejected
```

---

### Data & Privacy Settings Screen

Accessible from the study navigation under **Data & Privacy**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data & Privacy  Â·  Onboarding Feb 2026                       â”‚
â”‚                                                               â”‚
â”‚  Stored data                                                  â”‚
â”‚  5 anonymised transcripts  Â·  1 codebook  Â·  12 insights      â”‚
â”‚  Original transcripts: not stored                             â”‚
â”‚                                                               â”‚
â”‚  Participant ID mapping                                       â”‚
â”‚  You hold the mapping between IDs and real participants.      â”‚
â”‚  This tool does not store or have access to that mapping.     â”‚
â”‚                                                               â”‚
â”‚  Retention                                                    â”‚
â”‚  Auto-delete study data after:  [6 months â–¼]                  â”‚
â”‚                                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚                                                               â”‚
â”‚  [Delete all study data]                                      â”‚
â”‚  This permanently removes all transcripts, codes, insights,   â”‚
â”‚  and report data. This cannot be undone.                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Researcher-Facing Language

Avoid legalistic privacy copy. Use plain, honest language throughout:

| Instead of | Use |
|---|---|
| "We process your data in accordance with..." | "We anonymise before analysis. Originals are discarded." |
| "Consent confirmed" | "Participants were informed this interview may be AI-analysed" |
| "PII detected" | "Found information that could identify someone" |
| "GDPR compliant" | "Real names are never stored in this tool" |

A persistent indicator appears throughout the app wherever quotes or participant data are shown:

```
  ğŸ”’  All quotes anonymised  Â·  Original transcripts not stored
```

---



### Research Brief (YAML)
- Research objectives â€” the questions we need to answer
- Hypotheses to test
- Participant criteria and recruitment screener
- Expected session duration and format

### Interview Guide (YAML/JSON)

| Field | Description |
|---|---|
| Research questions | Top-level objectives the guide is designed to answer |
| Sections | Intro, warmup, core topics, concept test, closing |
| Per question | Text, required/optional, mapped research objective, probes |

AI uses this to score coverage, measure depth, detect off-script moments, and suggest improvements after each session.

### Transcript File

Raw conversational text exported from Askable or any transcription platform:

| Format | Source |
|---|---|
| `.vtt` | Zoom, Teams, Otter.ai, Whisper |
| `.srt` | Zoom, YouTube, older services |
| `.txt` | Otter.ai free, manual transcription |
| `.json` | Rev.ai API, AssemblyAI API |

All formats normalised to a canonical `Turn` structure: `{speaker, text, start, end, turn_index}`.

### Theme Guide / Codebook (YAML/JSON)

| Field | Description |
|---|---|
| Theme hierarchy | Top-level theme â†’ sub-themes â†’ codes |
| Per code | Definition, inclusion criteria, exclusion criteria |
| Indicators | Linguistic signals that suggest this code |
| Example quotes | Verbatim examples that exemplify the code |
| Code type | Deductive (hypothesis-driven) or emergent (discovered) |

---

## Core Analysis â€” Three-Way Triangulation

Every transcript is analysed against both structured inputs simultaneously:

```
Transcript Turns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                                                 â”‚
      â”‚    Interview Guide                              â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Guide Coverage Analysis      â”‚
      â”‚                     - Which Qs covered?         â”‚
      â”‚                     - Depth per question (1â€“5)  â”‚
      â”‚                     - Off-script classification â”‚
      â”‚                                                 â”‚
      â”‚    Theme Guide / Codebook                       â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Deductive Coding             â”‚
      â”‚                     - Apply codebook codes      â”‚
      â”‚                     - Confidence + evidence     â”‚
      â”‚                                                 â–¼
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Inductive Pass â”€â”€â–º GAP ANALYSIS REPORT
                            - Uncoded segments               â”‚
                            - Emergent themes                â”‚
                                                             â–¼
                                                  Feeds next session
                                                  prep + codebook
                                                  iteration
```

---

## Agent Orchestration Architecture

The workflow is implemented as a multi-agent system: one **Orchestrator** that manages study-level state and routes work to **Specialist Agents** that each own a single concern. Agents communicate via a shared `StudyState` object â€” not directly with each other.

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      ORCHESTRATOR        â”‚
                    â”‚                          â”‚
                    â”‚  - Manages StudyState    â”‚
                    â”‚  - Routes tasks          â”‚
                    â”‚  - Tracks saturation     â”‚
                    â”‚  - Surfaces human gates  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                    â–¼                    â–¼
   PRE-RESEARCH          PER-SESSION          CROSS-SESSION
   AGENTS                AGENTS               AGENTS
```

---

### Agent Registry

#### Pre-Research Agents (run once per study)

| Agent | Input | Output | Runs |
|---|---|---|---|
| **Brief Agent** | Product brief / HMW statements | Structured YAML research brief | Sequential first |
| **Guide Generator** | Research brief | Interview guide YAML + validation report | Sequential after Brief |
| **Codebook Seeder** | Brief + guide | Initial deductive codebook YAML | Sequential after Guide |

These three run sequentially â€” each output is ground truth for the next.

---

#### Per-Session Agents (after each Askable interview)

```
Transcript file
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PARSER    â”‚  â† runs first; all others depend on its output
â”‚   AGENT     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚  canonical Turn[]
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GUIDE COVERAGE  â”‚                â”‚  DEDUCTIVE CODER    â”‚
â”‚  AGENT           â”‚                â”‚  AGENT              â”‚
â”‚                  â”‚                â”‚                     â”‚
â”‚ - Qs hit/missed  â”‚                â”‚ - Apply codebook    â”‚
â”‚ - Depth score    â”‚                â”‚ - Confidence +      â”‚
â”‚ - Off-script     â”‚                â”‚   evidence quote    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚  coded + annotated turns
                            â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  INDUCTIVE          â”‚
                  â”‚  DISCOVERY AGENT    â”‚  â† runs on uncoded turns only
                  â”‚                     â”‚
                  â”‚  - Emergent themes  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  SESSION QA AGENT   â”‚
                  â”‚                     â”‚
                  â”‚  - Flags low-conf   â”‚
                  â”‚  - Quality scorecardâ”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  HUMAN REVIEW GATE  â”‚  â† researcher confirms flagged codes
                  â”‚                     â”‚     before anything is committed
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Guide Coverage Agent** and **Deductive Coder Agent** run in **parallel** â€” they are fully independent on the same parsed input. This is the biggest latency reduction in the pipeline.

---

#### Cross-Session Agents (after each reviewed session is committed)

| Agent | Role | Pattern |
|---|---|---|
| **Pattern Aggregator** | Updates theme heatmap across all sessions | Accumulator â€” diffs new session only |
| **Saturation Monitor** | Tracks theme emergence rate; fires signal when rate drops below threshold | Event-driven trigger |
| **Codebook Curator** | Recommends add / split / retire based on cross-session evidence | Runs every N sessions or on-demand |

---

#### Synthesis & Report Agents

| Agent | Role | Runs |
|---|---|---|
| **Insight Generator** | Ranked insight statements from confirmed patterns | On-demand after researcher signals ready |
| **Evidence Linker** | Attaches verbatim quotes + participant IDs to each insight | Sequential after Insight Generator |
| **Report Compiler** | Assembles final report structure | Sequential after Evidence Linker; human finalises |

---

### Claude Capability Mapping

Each agent uses the Claude capability best suited to its task â€” not every agent needs the most expensive call.

| Capability | Where applied | Why |
|---|---|---|
| **Extended thinking** | Brief Agent, Codebook Seeder, Insight Generator | Multi-step reasoning across competing evidence; weighing objective hierarchy; inferring observable indicators from abstract hypotheses |
| **Tool use** | All agents | File read/write, database queries, guide/codebook validation, heatmap queries |
| **Structured output (Pydantic)** | Every agent boundary | Agents communicate via validated schemas â€” never raw text. Orchestrator always knows the shape of what it receives |
| **Parallel subagent spawning** | Orchestrator, per session | Guide Coverage + Deductive Coder spawned simultaneously on same transcript. Also: multiple sessions can run in parallel on batch upload |
| **`temperature=0`** | Deductive Coder | Deterministic code application; reproducible results |
| **`temperature=0.3â€“0.5`** | Inductive Discovery, Insight Generator | Creative pattern recognition; emergent theme generation |

---

### Shared Study State

Agents do not call each other â€” they read from and write to a shared `StudyState`. The Orchestrator owns mutation.

```yaml
StudyState:
  study_id: str
  research_brief: ResearchBrief
  interview_guide: InterviewGuide
  codebook:
    version: int
    codes: Code[]
  sessions:
    - session_id: str
      participant_id: str
      transcript: Turn[]
      coverage_result: QuestionCoverageResult[]
      coded_turns: CodedTurn[]
      emergent_themes: EmergentTheme[]
      quality_scorecard: SessionScorecard
      review_status: pending | approved | rejected
  cross_session:
    theme_heatmap: ThemeMatrix
    saturation_curve: float[]        # new theme emergence rate per session
    participant_clusters: Cluster[]
  insights: InsightStatement[]
  report_draft: Report | null
```

Every agent output is a typed Pydantic model. The Orchestrator writes each model into the relevant `StudyState` field after validation.

---

### The Per-Session Feedback Loop

This is the highest-value feature in the system â€” it makes the research process smarter across sessions, not just within them.

```
Session N complete
      â”‚
      â–¼
Session QA Agent â†’ quality scorecard
      â”‚
      â–¼
Human review gate (researcher confirms/edits codes)
      â”‚
      â–¼
Pattern Aggregator updates StudyState
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NEXT SESSION PREP AGENT                          â”‚
â”‚                                                  â”‚
â”‚  Reads: current StudyState                       â”‚
â”‚  Outputs: briefing delivered to researcher       â”‚
â”‚  before next Askable interview                   â”‚
â”‚                                                  â”‚
â”‚  Example output:                                 â”‚
â”‚  "In Session N+1, prioritise:                    â”‚
â”‚   - FD1 (skipped in last 2 sessions)             â”‚
â”‚   - Probe OB2 deeper (avg depth 2.1/5)           â”‚
â”‚   - Test EMERGENT: export_pain (3/5 sessions,    â”‚
â”‚     not yet in codebook â€” propose adding)"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
Researcher reads brief before Askable interview
```

---

### Orchestration Patterns in Use

| Pattern | Where applied |
|---|---|
| **Sequential pipeline** | Brief â†’ Guide â†’ Codebook Seeder (pre-research); Parser â†’ Inductive â†’ QA (per session) |
| **Fan-out / fan-in** | Orchestrator fans out to Guide Coverage + Deductive Coder in parallel; fans back in when both complete |
| **Accumulator** | Pattern Aggregator processes only the diff from the new session â€” not all sessions each time |
| **Event-driven trigger** | Saturation Monitor watches emergence curve; fires when slope drops below threshold, prompting researcher |
| **Human-in-the-loop gate** | After Session QA Agent; orchestrator pauses, presents flagged items, waits for researcher confirmation before committing |
| **Feedback loop** | Session N output â†’ Next Session Prep Agent â†’ researcher brief â†’ improved Session N+1 |

---

## Build Phases

### Phase 1 â€” MVP (4â€“6 weeks)
*Goal: replace the manual "organise against guide" step with a robust analysis layer*

- Multi-format transcript parser (VTT, SRT, TXT, JSON)
- Auto-detect format + normalise to canonical `Turn` objects
- Interview guide upload (YAML/JSON) â€” questions, sections, probes
- Theme guide / codebook upload (YAML/JSON) â€” codes, definitions, indicators, examples
- Embedding-based pre-filter: match turns to questions + codes via cosine similarity (reduces LLM calls ~70%)
- LLM deductive coding: apply codebook codes to candidate turns â†’ confidence + evidence quote
- Guide coverage report: questions covered, skipped, shallow
- Basic annotated transcript view: turns highlighted by code, linked to source

**Stack:** FastAPI + Supabase (PostgreSQL + Auth + Storage) + Claude API + React

---

### Phase 2 â€” Core Product (6â€“8 weeks)
*Goal: enable cross-session analysis and the per-session feedback loop*

- LLM depth scoring per guide question (specificity, elaboration, emotional salience, actionability â€” 1â€“5 each)
- Off-script detection: classify uncovered segments as `productive_emergent | productive_rapport | unproductive_tangent | interviewer_error`
- Inductive pass: find recurring patterns in uncoded turns â†’ propose emergent themes
- Session quality scorecard (coverage %, avg depth, emergent theme count)
- Cross-session theme heatmap: theme Ã— participant occurrence matrix
- Saturation tracker: chart new theme emergence rate across sessions
- Codebook iteration recommendations: add / split / retire codes based on cross-session evidence
- Three-way gap analysis:
  - Guide gaps: questions asked but answered shallowly, or never asked
  - Codebook gaps: expected themes with zero evidence in transcript
  - Emergent gaps: patterns in transcript covered by neither guide nor codebook

---

### Phase 3 â€” Advanced (8â€“12 weeks)
*Goal: synthesis, reporting, and full research lifecycle support*

- AI-generated candidate insight statements ranked by frequency + salience + objective alignment
- Each insight: theme + definition + supporting quotes + participant count + confidence
- Participant segmentation: cluster participants by theme occurrence pattern
- Interviewer behaviour analysis: probe usage rate, leading question detection, missed probe flags
- Guide evolution workflow: accept/reject suggested additions to guide and codebook within the tool
- PII redaction before LLM processing (Microsoft Presidio)
- RAG-powered Q&A across all sessions: "Which participants mentioned workarounds for X?"
- Auto-generated report structure: Key Insights â†’ Observations â†’ Recommendations, all claims linked to coded evidence
- Export: annotated transcript HTML, gap analysis Markdown, updated codebook YAML draft, cross-session heatmap CSV

---

## Key Outputs

### Session Quality Scorecard
```
Guide coverage:     8/10 questions  (80%)
Avg depth score:    3.2/5.0
Useful off-script:  3 moments â†’ 2 worth adding to guide
Wasted time:        1 tangent (4 min)
Codebook coverage:  6/9 themes observed
Absent themes:      WORKAROUND_EXTERNAL, DELIGHT
Emergent themes:    2 new patterns found
```

### Per-Question Coverage Table
```
ID    Question                       Covered?  Depth  Notes
OB1   Walk me through first login    YES       4/5    Strong, specific narrative
OB2   Was there a stuck moment?      YES       2/5    Shallow â€” probe not used
FD1   How did you find features?     NO        â€”      Skipped â€” ran long on OB2
```

### Annotated Transcript
```
[00:12:34] P1: "I just kept clicking on different tabs hoping to find settings."
           â†’ CODES: [CONFUSION_NAV â˜…â˜…â˜…] [WORKAROUND_SELF â˜…â˜…]
           â†’ GUIDE: Covers OB2 (depth: 3/5)

[00:13:15] P1: "I honestly just Googled it."
           â†’ CODES: [WORKAROUND_EXTERNAL â˜…â˜…â˜…] [EMERGENT: external_support_preference]
```

### Cross-Session Theme Heatmap
```
                       P01  P02  P03  P04  P05  Total
CONFUSION_NAV           â–ˆâ–ˆ   â–ˆâ–ˆ   â–ˆ    â–ˆâ–ˆ   â–ˆâ–ˆ    9
CONFUSION_MENTAL_MODEL  â–ˆâ–ˆ   â–ˆ    â–ˆâ–ˆ   â€”    â–ˆ     6
WORKAROUND_EXTERNAL     â–ˆ    â€”    â–ˆâ–ˆ   â–ˆ    â€”     4
DELIGHT                 â€”    â–ˆ    â€”    â€”    â–ˆ     2
[EMERGENT] Export_pain  â–ˆ    â–ˆâ–ˆ   â€”    â–ˆâ–ˆ   â–ˆâ–ˆ    6  â† not in codebook
```

### Codebook Iteration Recommendations
```
ADD:    EXPORT_PAIN â€” appeared in 4/5 sessions, high salience, not in codebook
SPLIT:  CONFUSION_NAV â†’ separate nav confusion vs. IA confusion
RETIRE: DELIGHT â€” only 2/5 sessions, low confidence â†’ collapse into "Positive Moments"
PROBE:  Add to OB2 â€” "Did you look for help anywhere?" (surfaced organically in 3 sessions)
```

---

## Technical Stack

### Platform

| Component | Technology | Why |
|---|---|---|
| **Database** | Supabase (PostgreSQL) | Hosted Postgres with auth, file storage, and realtime â€” avoids managing infra |
| **File storage** | Supabase Storage | Transcript and guide uploads stored in buckets; accessed via signed URLs |
| **Auth** | Supabase Auth | Design team login; row-level security on study data |
| **Backend** | FastAPI | Python API layer between frontend and Claude; owns analysis orchestration |
| **Frontend** | React | Hosted web app used by the design team |
| **AI** | Claude API (`anthropic` SDK) | All analysis â€” guide parsing, transcript organisation, theme extraction, insight synthesis |
| **Validation** | Pydantic | Structured output validation on all AI responses |

### Parsing

| Library | Purpose |
|---|---|
| `webvtt-py` | VTT files |

### Hosting

The tool is a hosted web application used by the design team. Supabase handles data persistence, file storage, and authentication. The FastAPI backend and React frontend are deployed separately (hosting provider TBD).

---

## Key Algorithmic Decisions

### Chunking Granularity
Speaker turn is the natural coding unit. Avoid sentence-level chunking (loses context). For long monologues (>300 words), split on sentence boundaries into ~150-word windows with 50-word overlap.

### LLM Model Choice
- Claude Sonnet 4.6 for deductive coding â€” achieves Îº = 0.61â€“0.65 agreement with human coders
- Smaller models adequate for first-pass filtering, require human review on low-confidence codes
- `temperature=0` for deductive passes; `temperature=0.3â€“0.5` for emergent theme generation

### Embedding Pre-Filter
Before sending every turn to the LLM for every code (O(nÃ—m) calls), use `sentence-transformers` cosine similarity to pre-filter candidates. Only send turnâ†’code pairs where similarity > 0.3. Reduces LLM calls by 60â€“80% in practice.

### Human-in-the-Loop Gates
- Always expose confidence scores alongside coded output
- Surface turns with `confidence < 0.6` for researcher review before any code is accepted
- Never auto-accept low-confidence codes into permanent records
- Synthesis insights require human editorial review before report generation

### Caching
Cache LLM responses by `hash(segment_text + code_id + model_version)`. Rerunning after codebook updates only reprocesses turns affected by changed codes.

### Agent Isolation and Failure Handling
Each specialist agent is idempotent â€” it can be re-run on the same input without side effects. If an agent fails, the Orchestrator retries with exponential backoff (2s, 4s, 8s, 16s) before surfacing an error to the researcher. Partial results are never committed to `StudyState` â€” writes are transactional.

### Parallelism Budget
Running Guide Coverage and Deductive Coder in parallel doubles the Claude API concurrency per session. Set a per-study concurrency cap (default: 5 parallel agent calls) to avoid rate limit exhaustion on batch uploads of multiple sessions.

### Extended Thinking vs. Standard for Each Agent
Use extended thinking (`budget_tokens=8000`) only where multi-step reasoning across competing evidence is required:
- Brief Agent (objective hierarchy)
- Codebook Seeder (hypothesis â†’ observable indicator inference)
- Insight Generator (weighing frequency vs. salience vs. objective alignment)

Use standard (no thinking) for all other agents â€” deductive coding, parsing, guide coverage, report compilation. Thinking adds latency and cost where the task is structured and deterministic.

### Structured Output Contract
Every agent function has a typed signature: `agent_fn(input: AgentInput) -> AgentOutput`. The Orchestrator never passes raw text between agents. If a Claude response fails Pydantic validation, the Orchestrator retries once with an explicit correction prompt before raising to the researcher. This makes the pipeline auditable â€” every agent's output is a stored, versioned record.
